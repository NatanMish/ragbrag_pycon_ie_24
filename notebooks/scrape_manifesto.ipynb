{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scraped page 1\n",
      "Scraped page 2\n",
      "Scraped page 3\n",
      "Scraped page 4\n",
      "Scraped page 5\n",
      "Scraped page 6\n",
      "Scraped page 7\n",
      "Scraped page 8\n",
      "Scraped 185 policies and saved to loony_party_policies.txt\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import time\n",
    "\n",
    "def scrape_loony_party_policies():\n",
    "    base_url = \"https://www.loonyparty.com/about/policy-proposals/\"\n",
    "    page_number = 1\n",
    "    all_policies = []\n",
    "\n",
    "    while True:\n",
    "        if page_number == 1:\n",
    "            url = base_url\n",
    "        else:\n",
    "            url = f\"{base_url}page/{page_number}/\"\n",
    "\n",
    "        response = requests.get(url)\n",
    "        \n",
    "        if response.status_code != 200:\n",
    "            break\n",
    "\n",
    "        soup = BeautifulSoup(response.content, 'html.parser')\n",
    "        articles = soup.find_all('article')\n",
    "\n",
    "        if not articles:\n",
    "            break\n",
    "\n",
    "        for article in articles:\n",
    "            paragraphs = article.find_all('p')\n",
    "            for p in paragraphs:\n",
    "                text = p.get_text(strip=True)\n",
    "                if text:\n",
    "                    all_policies.append(text)\n",
    "\n",
    "        print(f\"Scraped page {page_number}\")\n",
    "        page_number += 1\n",
    "        time.sleep(1)  # Add a delay to be respectful to the server\n",
    "\n",
    "    return all_policies\n",
    "\n",
    "def main():\n",
    "    policies = scrape_loony_party_policies()\n",
    "    \n",
    "    if policies:\n",
    "        with open(\"../data/loony_party_policies.txt\", \"w\", encoding=\"utf-8\") as f:\n",
    "            for _, policy in enumerate(policies, 1):\n",
    "                f.write(f\"{policy}\\n\\n\")\n",
    "        print(f\"Scraped {len(policies)} policies and saved to loony_party_policies.txt\")\n",
    "    else:\n",
    "        print(\"No policies found or failed to scrape the website\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
