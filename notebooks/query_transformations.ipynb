{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Query Transformations\n",
    "\n",
    "### Imports and configs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/user/Projects/ragbrag_pycon_ie_24/venv/lib/python3.10/site-packages/pydantic/_internal/_fields.py:132: UserWarning: Field \"model_name\" in HuggingFaceInferenceAPIEmbeddings has conflict with protected namespace \"model_\".\n",
      "\n",
      "You may be able to resolve this warning by setting `model_config['protected_namespaces'] = ()`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading vector store from cache...\n"
     ]
    }
   ],
   "source": [
    "from typing import List\n",
    "from llama_index.core import SimpleDirectoryReader, VectorStoreIndex\n",
    "from llama_index.core.ingestion import IngestionPipeline\n",
    "from llama_index.core.schema import BaseNode, TransformComponent\n",
    "from llama_index.vector_stores.faiss import FaissVectorStore\n",
    "from llama_index.core.text_splitter import SentenceSplitter\n",
    "from llama_index.embeddings.openai import OpenAIEmbedding\n",
    "from llama_index.core import Settings\n",
    "from llama_index.core import PromptTemplate\n",
    "from llama_index.llms.openai import OpenAI\n",
    "import faiss\n",
    "import os\n",
    "import sys\n",
    "from dotenv import load_dotenv\n",
    "import hashlib\n",
    "import pickle\n",
    "\n",
    "\n",
    "sys.path.append(os.path.abspath(os.path.join(os.getcwd(), '..')))\n",
    "\n",
    "EMBED_DIMENSION = 512\n",
    "CHUNK_SIZE = 200\n",
    "CHUNK_OVERLAP = 50\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "os.environ[\"OPENAI_API_KEY\"] = os.getenv('OPENAI_API_KEY')\n",
    "\n",
    "Settings.embed_model = OpenAIEmbedding(model=\"text-embedding-3-small\", dimensions=EMBED_DIMENSION)\n",
    "\n",
    "path = \"../data/\"\n",
    "node_parser = SimpleDirectoryReader(input_dir=path, required_exts=['.txt', '.pdf'])\n",
    "documents = node_parser.load_data()\n",
    "### Set up vector store retriever\n",
    "class TextCleaner(TransformComponent):\n",
    "    \"\"\"\n",
    "    Transformation to be used within the ingestion pipeline.\n",
    "    Cleans clutters from texts.\n",
    "    \"\"\"\n",
    "    def __call__(self, nodes, **kwargs) -> List[BaseNode]:\n",
    "        \n",
    "        for node in nodes:\n",
    "            node.text = node.text.replace('\\t', ' ') # Replace tabs with spaces\n",
    "            node.text = node.text.replace(' \\n', ' ') # Replace paragraph seperator with spacaes\n",
    "            \n",
    "        return nodes\n",
    "CACHE_DIR = \"../cache\"\n",
    "VECTOR_STORE_PATH = os.path.join(CACHE_DIR, \"faiss_index.pkl\")\n",
    "HASH_PATH = os.path.join(CACHE_DIR, \"documents_hash.txt\")\n",
    "\n",
    "def hash_documents(documents):\n",
    "    # combine all the texts into a single string\n",
    "    all_titles = [doc.metadata['file_name'] for doc in documents]\n",
    "    all_titles_distinct = list(set(all_titles))\n",
    "    all_titles_distinct.sort()\n",
    "    all_titles_str = \" \".join(all_titles_distinct)\n",
    "    # return a hash of the combined text which will stay consistent if the text is the same across multiple runs\n",
    "    return hashlib.md5(all_titles_str.encode('utf-8')).hexdigest()\n",
    "\n",
    "def load_or_create_vector_store(documents, embed_dim, chunk_size, chunk_overlap):\n",
    "    os.makedirs(CACHE_DIR, exist_ok=True)\n",
    "    \n",
    "    current_hash = hash_documents(documents)\n",
    "    \n",
    "    if os.path.exists(HASH_PATH) and os.path.exists(VECTOR_STORE_PATH):\n",
    "        with open(HASH_PATH, 'r') as f:\n",
    "            stored_hash = f.read().strip()\n",
    "\n",
    "        if stored_hash == current_hash:\n",
    "            print(\"Loading vector store from cache...\")\n",
    "            with open(VECTOR_STORE_PATH, 'rb') as f:\n",
    "                return pickle.load(f)\n",
    "    \n",
    "    print(\"Creating new vector store...\")\n",
    "    faiss_index = faiss.IndexFlatL2(embed_dim)\n",
    "    vector_store = FaissVectorStore(faiss_index=faiss_index)\n",
    "    \n",
    "    text_splitter = SentenceSplitter(chunk_size=chunk_size, chunk_overlap=chunk_overlap)\n",
    "    \n",
    "    pipeline = IngestionPipeline(\n",
    "        transformations=[\n",
    "            TextCleaner(),\n",
    "            text_splitter,\n",
    "        ],\n",
    "        vector_store=vector_store,\n",
    "    )\n",
    "    \n",
    "    nodes = pipeline.run(documents=documents)\n",
    "    vector_store_index = VectorStoreIndex(nodes)\n",
    "    \n",
    "    # Save the new vector store and hash\n",
    "    with open(VECTOR_STORE_PATH, 'wb') as f:\n",
    "        pickle.dump(vector_store_index, f)\n",
    "    \n",
    "    with open(HASH_PATH, 'w') as f:\n",
    "        f.write(current_hash)\n",
    "    \n",
    "    return vector_store_index\n",
    "\n",
    "vector_store_index = load_or_create_vector_store(documents, EMBED_DIMENSION, CHUNK_SIZE, CHUNK_OVERLAP)\n",
    "retriever = vector_store_index.as_retriever(similarity_top_k=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Query transformations\n",
    "#### Query rewriting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "original query: What is the SNP's policy on climate change?\n",
      "generated query: What specific measures and initiatives does the Scottish National Party propose to address climate change, and how do they plan to implement these strategies within Scotland?\n",
      "Simple query response: The SNP's policy on climate change involves a commitment to tackling climate change and nature loss, emphasizing the importance of emissions reduction and economic prosperity going hand in hand. They aim to promote significant growth in renewables, storage, hydrogen, and carbon capture as the best pathway to achieving net zero emissions and ensuring secure, affordable, and clean energy. Additionally, the SNP wants to rule out new nuclear power plants in Scotland and promote Scotland's hydrogen export potential, pressing for progress in direct interconnection between Scotland and the continent to unlock Scotland's renewable potential.\n",
      "Improved query response: The Scottish National Party proposes to establish a Four Nations Climate Response Group to agree on climate plans across the UK and ensure the UK Government maintains its climate ambition. Additionally, they aim to devolve powers to create a bespoke migration system for Scotland that addresses specific demographic and economic needs, including introducing a rural visa pilot scheme. The party advocates for full powers over immigration for Scotland, including the devolution of overseas workers' employment visas. They also emphasize the importance of acknowledging Scotland's distinct demographic challenges and mitigating labor shortages through pilot schemes due to Brexit and immigration policies.\n"
     ]
    }
   ],
   "source": [
    "test_query = \"What is the SNP's policy on climate change?\"\n",
    "\n",
    "query_gen_str = \"\"\"\\\n",
    "You are an AI assistant tasked with reformulating user queries to improve retrieval in a RAG system. \n",
    "Given the original query, rewrite it to be more specific, detailed, and likely to retrieve relevant information.\n",
    "Original Query: {query}\n",
    "Rewritten Query:\"\"\"\n",
    "query_gen_prompt = PromptTemplate(query_gen_str)\n",
    "\n",
    "llm = OpenAI(model=\"gpt-4o\", temperature=0, max_tokens=4000)\n",
    "\n",
    "def generate_query(query: str, llm, query_gen_prompt):\n",
    "    response = llm.predict(\n",
    "        query_gen_prompt, query=query\n",
    "    )\n",
    "    # assume LLM proper put each query on a newline\n",
    "    new_query = response.split(\"\\n\")\n",
    "    query_str = \"\\n\".join(new_query)\n",
    "    return query_str\n",
    "\n",
    "generated_query = generate_query(test_query, llm, query_gen_prompt)\n",
    "print(f\"original query: {test_query}\")\n",
    "print(f\"generated query: {generated_query}\")\n",
    "# Compare improved query response\n",
    "query_engine = vector_store_index.as_query_engine()\n",
    "response_simple = query_engine.query(test_query)\n",
    "response_improved = query_engine.query(generated_query)\n",
    "print(f\"Simple query response: {response_simple}\")\n",
    "print(f\"Improved query response: {response_improved}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step-back Prompting: Generating broader queries for better context retrieval."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "original query: What is the SNP's policy on carbon emissions?\n",
      "generated query: What are the general policies and positions of the SNP on environmental issues?\n"
     ]
    }
   ],
   "source": [
    "test_query = \"What is the SNP's policy on carbon emissions?\"\n",
    "step_back_template = \"\"\"You are an AI assistant tasked with generating broader, more general queries to improve context retrieval in a RAG system.\n",
    "Given the original query, generate a step-back query that is more general and can help retrieve relevant background information.\n",
    "\n",
    "Original query: {query}\n",
    "\n",
    "Step-back query:\"\"\"\n",
    "query_gen_prompt = PromptTemplate(step_back_template)\n",
    "generated_query = generate_query(test_query, llm, query_gen_prompt)\n",
    "print(f\"original query: {test_query}\")\n",
    "print(f\"generated query: {generated_query}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Sub-query decomposition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "original query: What is the Conservative Party's stance on immigration?\n",
      "generated query: Sub-queries:\n",
      "1. What are the key policies of the Conservative Party regarding immigration?\n",
      "2. How has the Conservative Party's stance on immigration changed over recent years?\n",
      "3. What are the main arguments the Conservative Party uses to support its immigration policies?\n",
      "4. How do the Conservative Party's immigration policies compare to those of other major political parties?\n"
     ]
    }
   ],
   "source": [
    "test_query = \"What is the Conservative Party's stance on immigration?\"\n",
    "\n",
    "subquery_decomposition_template = \"\"\"You are an AI assistant tasked with breaking down complex queries into simpler sub-queries for a RAG system.\n",
    "Given the original query, decompose it into 2-4 simpler sub-queries that, when answered together, would provide a comprehensive response to the original query.\n",
    "\n",
    "Original query: {query}\n",
    "\n",
    "example: What are the impacts of climate change on the environment?\n",
    "\n",
    "Sub-queries:\n",
    "1. What are the impacts of climate change on biodiversity?\n",
    "2. How does climate change affect the oceans?\n",
    "3. What are the effects of climate change on agriculture?\n",
    "4. What are the impacts of climate change on human health?\"\"\"\n",
    "\n",
    "query_gen_prompt = PromptTemplate(subquery_decomposition_template)\n",
    "generated_query = generate_query(test_query, llm, query_gen_prompt)\n",
    "print(f\"original query: {test_query}\")\n",
    "print(f\"generated query: {generated_query}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
