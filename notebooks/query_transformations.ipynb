{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Query Transformations\n",
    "\n",
    "### Imports and configs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/user/Projects/ragbrag_pycon_ie_24/venv/lib/python3.10/site-packages/pydantic/_internal/_fields.py:132: UserWarning: Field \"model_name\" in HuggingFaceInferenceAPIEmbeddings has conflict with protected namespace \"model_\".\n",
      "\n",
      "You may be able to resolve this warning by setting `model_config['protected_namespaces'] = ()`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading vector store from cache...\n"
     ]
    }
   ],
   "source": [
    "from typing import List\n",
    "from llama_index.core import SimpleDirectoryReader\n",
    "from llama_index.embeddings.openai import OpenAIEmbedding\n",
    "from llama_index.core import Settings\n",
    "from llama_index.core import PromptTemplate\n",
    "from llama_index.llms.openai import OpenAI\n",
    "import os\n",
    "import sys\n",
    "from dotenv import load_dotenv\n",
    "import hashlib\n",
    "from utils import load_or_create_vector_store\n",
    "\n",
    "\n",
    "sys.path.append(os.path.abspath(os.path.join(os.getcwd(), '..')))\n",
    "\n",
    "EMBED_DIMENSION = 512\n",
    "CHUNK_SIZE = 250\n",
    "CHUNK_OVERLAP = 25\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "os.environ[\"OPENAI_API_KEY\"] = os.getenv('OPENAI_API_KEY')\n",
    "\n",
    "Settings.embed_model = OpenAIEmbedding(model=\"text-embedding-3-small\", dimensions=EMBED_DIMENSION)\n",
    "\n",
    "path = \"../data/\"\n",
    "node_parser = SimpleDirectoryReader(input_dir=path, required_exts=['.txt', '.pdf'])\n",
    "documents = node_parser.load_data()\n",
    "### Set up vector store retriever\n",
    "CACHE_DIR = \"../cache\"\n",
    "VECTOR_STORE_PATH = os.path.join(CACHE_DIR, \"faiss_index.pkl\")\n",
    "HASH_PATH = os.path.join(CACHE_DIR, \"documents_hash.txt\")\n",
    "\n",
    "def hash_documents(documents):\n",
    "    # combine all the texts into a single string\n",
    "    all_titles = [doc.metadata['file_name'] for doc in documents]\n",
    "    all_titles_distinct = list(set(all_titles))\n",
    "    all_titles_distinct.sort()\n",
    "    all_titles_str = \" \".join(all_titles_distinct)\n",
    "    # return a hash of the combined text which will stay consistent if the text is the same across multiple runs\n",
    "    return hashlib.md5(all_titles_str.encode('utf-8')).hexdigest()\n",
    "\n",
    "vector_store_index = load_or_create_vector_store(documents, EMBED_DIMENSION, CHUNK_SIZE, CHUNK_OVERLAP, cache_dir=CACHE_DIR, vector_store_path=VECTOR_STORE_PATH, hash_path=HASH_PATH)\n",
    "retriever = vector_store_index.as_retriever(similarity_top_k=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Query transformations\n",
    "#### Query rewriting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "original query: What is the SNP's policy on climate change?\n",
      "generated query: What specific measures and initiatives does the Scottish National Party (SNP) propose in their policy to address climate change, including their targets for reducing carbon emissions and transitioning to renewable energy sources?\n",
      "Simple query response: The SNP's policy on climate change includes banning new coal licenses, ensuring fair funding for climate initiatives, establishing a Four Nations Climate Response Group to meet net-zero targets, devolving powers for a bespoke migration system, mitigating the harm of Brexit on productivity, providing sustainable funding for farming, and giving Scotland its rightful share of marine funding.\n",
      "Improved query response: The Scottish National Party (SNP) proposes specific measures and initiatives to address climate change by advocating for an immediate emergency budget to reverse cuts to public spending and invest in green energy, supporting projects like the Acorn carbon capture and storage initiative, modernizing renewable energy schemes, taking an evidence-based approach to oil and gas extraction, supporting a just transition for North Sea oil and gas workers, securing the future of the Grangemouth complex, banning new coal licenses, ensuring fair funding for climate initiatives, establishing a Four Nations Climate Response Group, devolving powers for a bespoke migration system, mitigating Brexit's impact on productivity, providing sustainable funding for farming, and securing Scotland's share of marine funding. Their targets include reaching net zero emissions and transitioning to renewable energy sources to align with climate obligations and economic prosperity.\n"
     ]
    }
   ],
   "source": [
    "test_query = \"What is the SNP's policy on climate change?\"\n",
    "\n",
    "query_gen_str = \"\"\"\\\n",
    "You are an AI assistant tasked with reformulating user queries to improve retrieval in a RAG system. \n",
    "Given the original query, rewrite it to be more specific, detailed, and likely to retrieve relevant information.\n",
    "Original Query: {query}\n",
    "Rewritten Query:\"\"\"\n",
    "query_gen_prompt = PromptTemplate(query_gen_str)\n",
    "\n",
    "llm = OpenAI(model=\"gpt-4o\", temperature=0, max_tokens=4000)\n",
    "\n",
    "def generate_query(query: str, llm, query_gen_prompt):\n",
    "    response = llm.predict(\n",
    "        query_gen_prompt, query=query\n",
    "    )\n",
    "    return response\n",
    "\n",
    "generated_query = generate_query(test_query, llm, query_gen_prompt)\n",
    "print(f\"original query: {test_query}\")\n",
    "print(f\"generated query: {generated_query}\")\n",
    "# Compare improved query response\n",
    "query_engine = vector_store_index.as_query_engine()\n",
    "response_simple = query_engine.query(test_query)\n",
    "response_improved = query_engine.query(generated_query)\n",
    "print(f\"Simple query response: {response_simple}\")\n",
    "print(f\"Improved query response: {response_improved}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step-back Prompting: Generating broader queries for better context retrieval."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "original query: What is the SNP's policy on carbon emissions?\n",
      "generated query: What are the general policies and positions of the SNP on environmental issues?\n"
     ]
    }
   ],
   "source": [
    "test_query = \"What is the SNP's policy on carbon emissions?\"\n",
    "step_back_template = \"\"\"You are an AI assistant tasked with generating broader, more general queries to improve context retrieval in a RAG system.\n",
    "Given the original query, generate a step-back query that is more general and can help retrieve relevant background information.\n",
    "\n",
    "Original query: {query}\n",
    "\n",
    "Step-back query:\"\"\"\n",
    "query_gen_prompt = PromptTemplate(step_back_template)\n",
    "generated_query = generate_query(test_query, llm, query_gen_prompt)\n",
    "print(f\"original query: {test_query}\")\n",
    "print(f\"generated query: {generated_query}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Sub-query decomposition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "original query: What is the Conservative Party's stance on immigration?\n",
      "generated query: Sub-queries:\n",
      "1. What are the key policies of the Conservative Party regarding immigration?\n",
      "2. How has the Conservative Party's stance on immigration evolved over recent years?\n",
      "3. What are the main arguments the Conservative Party uses to support its immigration policies?\n",
      "4. How do the Conservative Party's immigration policies compare to those of other major political parties?\n"
     ]
    }
   ],
   "source": [
    "test_query = \"What is the Conservative Party's stance on immigration?\"\n",
    "\n",
    "subquery_decomposition_template = \"\"\"You are an AI assistant tasked with breaking down complex queries into simpler sub-queries for a RAG system.\n",
    "Given the original query, decompose it into 2-4 simpler sub-queries that, when answered together, would provide a comprehensive response to the original query.\n",
    "\n",
    "Original query: {query}\n",
    "\n",
    "example: What are the impacts of climate change on the environment?\n",
    "\n",
    "Sub-queries:\n",
    "1. What are the impacts of climate change on biodiversity?\n",
    "2. How does climate change affect the oceans?\n",
    "3. What are the effects of climate change on agriculture?\n",
    "4. What are the impacts of climate change on human health?\"\"\"\n",
    "\n",
    "query_gen_prompt = PromptTemplate(subquery_decomposition_template)\n",
    "generated_query = generate_query(test_query, llm, query_gen_prompt)\n",
    "print(f\"original query: {test_query}\")\n",
    "print(f\"generated query: {generated_query}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
