<!doctype html>
<html lang="en">
	<head>
		<meta charset="utf-8">
		<meta name="viewport" content="width=device-width, initial-scale=1.0, maximum-scale=1.0, user-scalable=no">

		<title>Build a RAG to Brag About</title>

		<link rel="stylesheet" href="dist/reset.css">
		<link rel="stylesheet" href="dist/reveal.css">
		<link rel="stylesheet" href="dist/theme/dracula.css">

		<!-- Theme used for syntax highlighted code -->
		<link rel="stylesheet" href="plugin/highlight/monokai.css">
		<script src="plugin/highlight/highlight.js"></script>
	</head>
	<body>
		<div class="reveal">
			<div class="slides">
				<section>
					<h2 class="r-fit-text">Build a RAG to Brag About</h2>
					<p>Natan Mish</p>
					<p style="font-size: large;">GitHub repository: <a href="https://github.com/NatanMish/ragbrag_pycon_ie_24/">https://github.com/NatanMish/ragbrag_pycon_ie_24/</a></p>
					<img src="https://s3.python.ie/images/image_6.width-500.png" alt="PyCon Ireland Logo" style="width: auto; height: auto;">
				</section>
				<section>
					<h2>Agenda</h2>
					<ul style="font-size: medium;">
						<li>Introduction</li>
						<li>Tools</li>
						<li>Data</li>
						<li>What is RAG?</li>
						<ul>
							<li>Use Cases</li>
							<li>Architecture</li>
							<li>Build a simple RAG with Llamaindex and FAISS</li>
							<li>What are the limitations of the standard RAG?</li>
						</ul>
						<li>Methods for improving RAG</li>
						<ul>
							<li>Hypothetical Document Embedding (HyDE)</li>
							<li>Query Transformations</li>
							<li>Contextual Chunk Headers</li>
							<li>Semantic Chunking</li>
						</ul>
						<!-- <li>Evaluation</li> -->
						<!-- <li>Other methods</li> -->
						<!-- <li>Why not fine-tune?</li>
						<li>Future of RAG</li> -->
						<li>Q&A</li>
					</ul>
					<h4>Hopefully, something to learn for everyone!</h4>
				</section>
				<section>
					<h2>About Me</h2>
					<p align="left" style="font-size: x-large;">
						Hello! I'm Natan Mish, a Machine Learning Engineer at <a href="https://www.zimmerbiomet.com/en">Zimmer Biomet</a>. My academical background is in Data Science and Economics and have worked in various industries, including finance, telecommunications, and transportation.
					</p>
					<p align="left" style="font-size: x-large;">
						I am a member of the scoping committee for <a href="https://datakind.org.uk/">DataKind UK</a>, a charity that helps other charities use data science to improve their operations.
					</p>
					<p align="left" style="font-size: x-large;">
						Python conferences are my favorite events, and I have spoken at <a href="https://pydata.org/london2024/">PyData London</a>, <a href="https://ep2024.europython.eu/">EuroPython</a> and this is my first time at PyCon Ireland!
					</p>
					<h4>Scan to Connect</h4>
					<img src="images/QR_code.JPG" alt="QR Code" style="width: 150px; height: auto;">
					<img src="https://media.licdn.com/dms/image/v2/D4E03AQHfhD2GwJw0ZA/profile-displayphoto-shrink_800_800/profile-displayphoto-shrink_800_800/0/1685690405626?e=1735171200&v=beta&t=C1dpEBPwaZPsY8JOobfnQMlE4yWnJkeIAoqsoi3u41c" alt="My Picture" style="width: 150px; height: auto;">
				</section>
				<section>
					<h2>Tools</h2>
					<ul>
						<li style="display: flex; align-items: center;">
							Jupyter Notebooks 
							<img src="https://encrypted-tbn0.gstatic.com/images?q=tbn:ANd9GcSn5qoh2GS_WbBtD2Zz6I9z8JaagZ9zEoLlNw&s" alt="Jupyter Logo" style="width: 40px; height: auto; margin-left: 10px;">
						</li>
						<li style="display: flex; align-items: center;">
							Llamaindex 
							<img src="https://media.licdn.com/dms/image/v2/D560BAQFbVTDw-oWXmw/company-logo_200_200/company-logo_200_200/0/1681327675102?e=2147483647&v=beta&t=R9OFpwpWNWEe7kwjOsCxMTYn2gpdxcpUTGUjm40cpvw" alt="Llamaindex Logo" style="width: 40px; height: auto; margin-left: 10px;">
						</li>
						<ul style="font-size: small;">
							<li>Alternatively: LangChain, Haystack</li>
						</ul>
						<li style="display: flex; align-items: center;">
							FAISS
							<img src="https://encrypted-tbn0.gstatic.com/images?q=tbn:ANd9GcSd69cIjDCayFutPxAvAzoAbaj6LMyEICEgEw&s" alt="FAISS Logo" style="width: 40px; height: auto; margin-left: 10px;">
						</li>
						<ul style="font-size: small;">
							<li>Alternatively: Qdrant, Pinecone, Chroma</li>
						</ul>
						<li style="display: flex; align-items: center;">
							OpenAI
							<img src="https://encrypted-tbn0.gstatic.com/images?q=tbn:ANd9GcR2d3IW4R8PR4TO7Va-lOAV6PrrYh250bqJpw&s" alt="OpenAI Logo" style="width: 40px; height: auto; margin-left: 10px;">
						</li>
						<ul style="font-size: small;">
							<li>Alternatively: Anthropic, Hugging Face</li>
					</ul>
				</section>
				<section>
					<section>
					<h3>UK General Elections 2024 Manifestos</h3>
					<img src="https://static01.nyt.com/images/2024/08/04/multimedia/04uk-blog-antics-03-plvt/04uk-blog-antics-03-plvt-googleFourByThree.jpg" alt="UK General Elections 2024" style="width: 500px; height: auto;">
					</section>
					<section>
						<img src="https://www.caabu.org/sites/default/files/styles/large/public/images/visual/polparties.png?itok=wxAeI-lm" alt="UK Political Parties" style="width: 500px; height: auto;">
					</section>
					<section>
						<h3>Parsing is a pain</h3>
						<p style="font-size: medium;">Files can be stored in different formats, such as PDF, Word, HTML, and plain text. Some context is inferred from the structure of the document, such as headings, bullet points, and tables. The text can be extracted using OCR, but the structure might be lost.</p>
						<p style="font-size: medium;">We will demonstrate some tools that can overcome these challenges and help you build a RAG to Brag About.</p>
						<p style="display: flex; justify-content: space-between;">
							<img src="images/pdf_screenshot_1.png" alt="PDF Screenshot" style="width: 49%; height: 49%;">
							<img src="images/pdf_screenshot_2.png" alt="PDF Screenshot" style="width: 25%; height: 25%;">
						</p>
					</section>
				</section>
				<section>
					<section>
						<h3>Everyones talking about RAG</h3>
						<iframe src="https://giphy.com/embed/H3LbIGBsYsEukxDVfR" width="480" height="269" style="" frameBorder="0" class="giphy-embed" allowFullScreen></iframe><p>Is it though? I'll let you decide</p>
					</section>
					<section>
						<h3>Retrieval Augmented Generation</h3>
						<p>RAG is an AI framework that combines the strengths of traditional information retrieval systems (such as search and databases) with the capabilities of generative large language models (LLMs). By combining your data and world knowledge with LLM language skills, grounded generation is more accurate, up-to-date, and relevant to your specific needs.</p>
					</section>
					<section>
						<h3>Use Cases</h3>
						<ul>
							<li>
								<p>Medical</p>
							</li>
							<li>
								<p>Legal</p>
							</li>
							<li>
								<p>Finance</p>
							</li>
							<li>
								<p>Education</p>
							</li>
						</ul>
					</section>
					<section>
						<h3>Architecture</h3>
						<img src="images/RAG_architecture.png" alt="RAG Architecture" style="width: 500px; height: auto;">
					</section>
					<section>
						<h3>Time to get our hands dirty</h3>
						<p><img src="https://media1.tenor.com/m/Puad53yl9J8AAAAC/the-wire-michael-k-williams.gif" alt="Michael K Williams" style="width: 500px; height: auto;"></p>
					</section>
					<section>
						<h2>Configuration</h2>
						<p>
							<pre><code data-trim class="language-python" data-noescape data-line-numbers="">
								from llama_index.core import Settings
								# The embedding dimension is the size of the vector 
								# representation of the text
								EMBED_DIMENSION = 512

								# Load environment variables from a .env file
								load_dotenv()

								# Set embedding model on LlamaIndex global settings
								Settings.embed_model = OpenAIEmbedding(
									model="text-embedding-3-small", 
									dimensions=EMBED_DIMENSION
								)
							</code>
							</pre>
						</p>
					</section>
					<section>
						<h2>Read in the documents</h2>
						<p>
							<pre><code class="language-python" data-trim data-noescape data-line-numbers="">
								from llama_index.core import SimpleDirectoryReader
								path = "../data/"
								node_parser = SimpleDirectoryReader(
									input_dir=path, 
									required_exts=['.txt', '.pdf']
								)
								documents = node_parser.load_data()
								print(documents[0])
							</code>
							</pre>
						</p>
						<p>
							<pre><code class="language-shell" data-trim data-noescape>
								Doc ID: 71419e08-91cf-4966-a1d6-7e6a5fb8b20f
								Text: Promoted by SNP 3 Jacksons Entry EH8 8PJ. Printed by Saltire 60
								Brook Street G40 2AB.“A FUTURE    MADE IN    SCOTLAND.” VOTE SNP  FOR
								SCOTLAND
							</code></pre>
						</p>
					</section>
					<section>
						<h2>Text Cleaning</h2>
						<p>
							<pre><code class="language-python" data-trim data-noescape data-line-numbers="">
								from llama_index.core.schema import BaseNode, TransformComponent

								class TextCleaner(TransformComponent):
									def __call__(self, nodes, **kwargs) -> List[BaseNode]:
										
										for node in nodes:
											# Replace tabs with spaces
											node.text = node.text.replace('\t', ' ') 
											# Replace paragraph separator with spaces
											node.text = node.text.replace(' \n', ' ')
											
										return nodes
							</code>
							</pre>
					</section>
					<section>
						<h3>Create Vector Store and Query Engine</h3>
						<p>
							<pre><code class="language-python" data-trim data-noescape data-line-numbers="">
								from llama_index.vector_stores.faiss import FaissVectorStore
								from llama_index.core.text_splitter import SentenceSplitter
								from llama_index.core.ingestion import IngestionPipeline

								EMBED_DIMENSION = 512
								CHUNK_SIZE = 250
								CHUNK_OVERLAP = 25

								faiss_index = faiss.IndexFlatL2(EMBED_DIMENSION)
								vector_store = FaissVectorStore(faiss_index=faiss_index)
								
								text_splitter = SentenceSplitter(
									chunk_size=CHUNK_SIZE,
									chunk_overlap=CHUNK_OVERLAP
								)
								
								pipeline = IngestionPipeline(
									transformations=[
										TextCleaner(),
										text_splitter,
									],
									vector_store=vector_store,
								)
								
								nodes = pipeline.run(documents=documents)
								vector_store_index = VectorStoreIndex(nodes)
								query_engine = vector_store_index.as_query_engine(similarity_top_k=2)
							</code>
							</pre>
						</p>
					</section>
					<section>
						<h3>Submit a Query</h3>
						<p>
							<pre><code class="language-python" data-trim data-noescape data-line-numbers="">
								test_query = "What is the SNP's policy on climate change?"
								results = query_engine.query(test_query)
								print(results)
							</code>
							</pre>
						</p>
						<p>
							<pre><code class="language-shell" data-trim data-noescape>
								The SNP's policy on climate change includes banning new coal 
								licenses, ensuring fair funding for climate initiatives, 
								establishing a Four Nations Climate Response Group to meet 
								net-zero targets, devolving powers for a bespoke migration 
								system, mitigating the harm of Brexit on productivity, 
								providing sustainable funding for farming, and giving 
								Scotland its rightful share of marine funding.
							</code></pre>
				</section>
				<section>
					<h3>Limitations of the standard RAG</h3>
					<ul>
						<li>Inaccurate or Irrelevant Results</li>
						<ul style="font-size: medium;">
							<li>Missing Critical Information: Essential documents containing the answer may not be included in the top retrieval results</li>
						</ul>
					<li>Domain Adaptability</li>
						<ul style="font-size: medium;">
							<li>Making the RAG model work well for multiple different domains is a challenge</li>
						</ul>
					<li>Scalability</li>
						<ul style="font-size: medium;">
							<li>As the number of documents increases, the retrieval time increases</li>
							<li>Underlying LLM models might have limited availability</li>
						</ul>
					</ul>
					</section>
				</section>
				<section>
					<h3>How can we improve our RAG?</h3>
					<img src="https://media1.giphy.com/media/v1.Y2lkPTc5MGI3NjExMDZra3AyeWU1dzU4Z3V0cHd0cGpxZmhiZjExNGVwcGs3Mzd4ZHM1cSZlcD12MV9pbnRlcm5hbF9naWZfYnlfaWQmY3Q9Zw/BpGWitbFZflfSUYuZ9/giphy.webp" alt="Improve RAG" style="width: 500px; height: auto;">
				</section>
				<section>
					<section>
					<h3>Hypothetical Document Embedding (HyDE)</h3>
					<p align="left" style="font-size: large;">HyDE aims to improve document retrieval by generating a hypothetical document based on the user's query. This approach is designed to bridge the gap between short, potentially imperfect user queries and longer, well-written documents containing the relevant information</p>
					<p>How HyDE Works</p>
					<ol align="left" style="font-size: large;">
						<li><strong>Hypothetical Document Generation:</strong> When a user submits a query, HyDE uses a language model (like GPT-3) to generate a hypothetical document that represents an ideal answer to the query.</li>
						<li><strong>Embedding Generation:</strong> This hypothetical document is then converted into an embedding vector using a pre-trained embedding model.</li>
						<li><strong>Similarity Search:</strong> The system searches for real documents in the corpus that are most similar to the encoded hypothetical document, rather than directly matching the original query.</li>
					</ol>
					</section>
					<section>
						<img src="../images/HYDE_architecture.png" alt="HyDE Architecture" style="width: auto; height: auto;">
					</section>
					<section>
						<p>
							<pre><code class="language-python" data-trim data-noescape data-line-numbers="1-4|6-12|13-21|23-29|30-33|35-40|41-50">
								from llama_index.embeddings.openai import OpenAIEmbedding
								from llama_index.core.query_pipeline import QueryPipeline
								from llama_index.core import PromptTemplate
								from llama_index.llms.openai import OpenAI

								class HyDERetriever:
									def __init__(
										self, 
										chunk_size=250, 
										chunk_overlap=50, 
										retriever=None
									):
										self.llm = OpenAI(
											temperature=0, 
											model_name="gpt-4o", 
											max_tokens=4000
										)
										self.embeddings = Settings.embed_model
										self.chunk_size = chunk_size
										self.chunk_overlap = chunk_overlap
										self.vectore_store_retriever = retriever    
										
										self.hyde_prompt = PromptTemplate(
											"""Given the question '{query}', generate a 
											hypothetical document that directly answers this 
											question. The document should be detailed and 
											in-depth.the document size has be exactly 
											{chunk_size} characters.""",
										)
										self.hyde_chain = QueryPipeline(
											chain=[self.hyde_prompt, self.llm], 
											verbose=True
										)

									def generate_hypothetical_document(self, query):
										return self.hyde_chain.run(
											query=query, 
											chunk_size=self.chunk_size
										)

									def retrieve(self, query):
										hypothetical_doc = self.generate_hypothetical_document(
											query
										)
										similar_docs = self.vectore_store_retriever.retrieve(
											query
										)
										return similar_docs, hypothetical_doc
							</code>
						</pre>
						</pre>
						</p>
					</section>
					<section>
						<h3>Hypothetical Document Example</h3>
						<p>
							<pre><code class="language-python" data-trim data-noescape data-line-numbers="">
								retriever = vector_store_index.as_retriever(similarity_top_k=2)

								hyde_retriever = HyDERetriever(
									chunk_size=250, 
									chunk_overlap=50, 
									retriever=retriever
								)

								results, hypothetical_doc = hyde_retriever.retrieve(test_query)
							</code>
							</pre>
						</p>
					</section>
					<section>
						<h4>Generated Hypothetical Document</h4>
						<p style="font-size: small;" align="left">
							The Scottish National Party (SNP) has a strong commitment to tackling climate change and has set ambitious targets to reduce greenhouse gas emissions. The SNP's policy on climate change is centered around transitioning to a low-carbon economy, investing in renewable energy sources, and promoting sustainable practices.One of the key initiatives of the SNP is the Climate Change (Emissions Reduction Targets) (Scotland) Act, which sets legally binding targets to reduce emissions by at least 75% by 2030 and achieve net-zero emissions by 2045. The SNP also supports the development of renewable energy sources such as wind, solar, and hydro power, with the goal of generating 100% of Scotland's electricity from renewable sources by 2020.In addition to reducing emissions and promoting renewable energy, the SNP is committed to promoting sustainable practices in areas such as transportation, agriculture, and land use. The SNP has introduced policies to encourage the use of electric vehicles, support sustainable farming practices, and protect Scotland's natural environment.Overall, the SNP's policy on climate change is comprehensive and ambitious, with a focus on reducing emissions, promoting renewable energy, and fostering sustainable practices across all sectors of the economy.
						</p>
					</section>
					<section>
						<h4>Retrieved Documents</h4>
						<p align="left">
							<li style="font-size: small;">DECISIONS MADE IN SCOTLAND, FOR SCOTLAND.      21SNP General Election Manifesto  2024Ban new coal licences. Follow the SNP Scottish Government’s lead and commit to no support for new coal mines, which would undermine  our action to reach net zero.Provide fair funding for climate. Scotland has over two thirds of the UK’s peatland, and currently plants over 60% of trees in the UK, yet funds restoration and planting within our budget, with no help from the UK Government. Westminster must ensure fair funding flows to devolved nations to enable our, and their, climate ambition given that for the whole of the UK to reach net zero by 2050, Scotland must do so  by 2045.Establish a Four Nations Climate Response Group to agree climate plans across the UK  that deliver on our net-zero targets and ensure  the UK Government stops backtracking on climate ambition.</li>
							<li style="font-size: small;">20BUILDING A FAIRER, GREENER ECONOMY Under the SNP, Scotland’s economy is already one of the best performing parts of the UK with both GDP per head and productivity growing faster in Scotland than the UK as a whole.  But we want to go further. Our commitment to tackling the twin crises of climate change and nature loss is unwavering and we believe emissions reduction and economic prosperity go hand in hand. We want  to share in the enormous economic opportunities of the global transition to net zero. SNP MPs will demand the UK Government:Bring forward an immediate emergency budget following the election to reverse cuts to public spending and deliver meaningful investment in economic growth, including green energy.Work at pace with the Acorn Project and Scottish Cluster to secure the fastest possible deployment following the UK Government’s failure to support the Acorn carbon capture, utilisation and storage project at track 1.</li>
						</p>
					</section>
					<section>
						<p>
							<img src="https://media4.giphy.com/media/v1.Y2lkPTc5MGI3NjExa3RvczF0Y2VodmJxbWxiM25lNm9qMXM1b2FscDd3dDRmZHZxeXJkbCZlcD12MV9pbnRlcm5hbF9naWZfYnlfaWQmY3Q9Zw/3o6vY2Ef4cKfRf8loc/giphy.webp" alt="What Could Go Wrong?" style="width: 500px; height: auto;">
						</p>
					</section>
					<section>
						<h3>LLM's already have some knowledge in them...</h3>
						<p align="left">If some or all of the information you are after is publicly available, the LLM might have been trained on it. This means that the generated document will be based on publicly available information and could potentially bias the results. One way to mitigate this is to configure the retrieval process to retrieve as many document as possible</p>
					</section>
				</section>
				<section>
					<section>
						<h3>Query Transformations</h3>
						<p style="font-size: large;">
							RAG systems often face challenges in retrieving the most relevant information, especially when dealing with complex or ambiguous queries. These query transformation techniques address this issue by reformulating queries to better match relevant documents or to retrieve more comprehensive information.
						</p>
						<p>
							<ul style="font-size: large;">
								<li>Query Rewriting</li>
								<li>Step-back Prompting</li>
								<li>Sub-query Decomposition</li>
							</ul>
						</p>
					</section>
					<section>
						<h3>Query Rewriting</h3>
						<p align="left" style="font-size: large;">
							Query rewriting involves transforming the original query into multiple variations that are more likely to retrieve relevant documents. This technique can be used to expand the scope of the query, add synonyms or related terms, or rephrase the query to improve retrieval accuracy.
						</p>
						<p>
							<pre>
								<code class="language-python" data-trim data-noescape data-line-numbers="1-2|4|6-12|14-15|17-21|23">
									from llama_index.core import PromptTemplate
									from llama_index.llms.openai import OpenAI

									test_query = "What is the SNP's policy on climate change?"

									query_gen_str = """\
									You are an AI assistant tasked with reformulating user 
									queries to improve retrieval in a RAG system. Given the 
									original query, rewrite it to be more specific, detailed, 
									and likely to retrieve relevant information.
									Original Query: {query}
									Rewritten Query:"""

									query_gen_prompt = PromptTemplate(query_gen_str)
									llm = OpenAI(model="gpt-4o", temperature=0, max_tokens=4000)

									def generate_query(query: str, llm, query_gen_prompt):
										response = llm.predict(
											query_gen_prompt, query=query
										)
										return response

									generated_query = generate_query(test_query, llm, query_gen_prompt)
								</code>
							</pre>
						</p>
					</section>
					<section>
						<h4>Original Query</h4>
						<p align="left">
							What is the SNP's policy on climate change?
						</p>
						<h4>Generated Query</h4>
						<p align="left">
							What specific measures and initiatives does the Scottish National Party (SNP) propose in their policy to address climate change, including their targets for reducing carbon emissions and transitioning to renewable energy sources?
						</p>
					</section>
					<section>
						<h3>Step-back Prompting</h3>
						<p align="left" style="font-size: large;">
							Step-back prompting generates broader, more general queries that can help retrieve relevant background information.
						</p>
						<p>
							<pre>
								<code class="language-python" data-trim data-noescape data-line-numbers="">
									test_query = "What is the SNP's policy on carbon emissions?"

									step_back_template = """You are an AI assistant tasked 
									with generating broader, more general queries to improve 
									context retrieval in a RAG system. Given the original 
									query, generate a step-back query that is more general 
									and can help retrieve relevant background information.
									Original query: {query}
									Step-back query:"""

									query_gen_prompt = PromptTemplate(step_back_template)

									generated_query = generate_query(test_query, llm, query_gen_prompt)
								</code>
							</pre>
						</p>
					</section>
					<section>
						<h4>Original Query</h4>
						<p align="left">
							What is the SNP's policy on carbon emissions?
						</p>
						<h4>Generated Query</h4>
						<p align="left">
							What are the general policies and positions of the SNP on environmental issues?
						</p>
					</section>
					<section>
						<h3>Sub-query Decomposition</h3>
						<p align="left" style="font-size: large;">
							Sub-query decomposition involves breaking down the original query into smaller, more specific sub-queries that can be used to retrieve relevant information. This technique can help address complex or ambiguous queries by focusing on individual components of the query.
						</p>
						<p>
							<pre>
								<code class="language-python" data-trim data-noescape data-line-numbers="">
									test_query = "What is the Conservative Party's stance on
									immigration?"

									subquery_decomposition_template = """You are an AI assistant 
									tasked with breaking down complex queries into simpler 
									sub-queries for a RAG system. Given the original query,
									decompose it into 2-4 simpler sub-queries that, when 
									answered together, would provide a comprehensive response to 
									the original query.
									Original query: {query}
									example: What are the impacts of climate change on the 
									environment?
									Sub-queries:
									1. What are the impacts of climate change on biodiversity?
									2. How does climate change affect the oceans?
									3. What are the effects of climate change on agriculture?
									4. What are the impacts of climate change on human health?"""

									query_gen_prompt = PromptTemplate(subquery_decomposition_template)
									generated_query = generate_query(test_query, llm, query_gen_prompt)
								</code>
							</pre>
						</p>
					</section>
					<section>
						<h4>Original Query</h4>
						<p align="left">
							<ul style="font-size: large;">
							<li>What is the Conservative Party's stance on immigration?</li>
							</ul>
						</p>
						<h4>Generated Sub-Queries</h4>
						<p align="left">
							<ul style="font-size: large;">
								<li> What are the key policies of the Conservative Party regarding immigration? </li>
								<li> How has the Conservative Party's stance on immigration evolved over recent years? </li>
								<li> What are the main arguments the Conservative Party uses to support its immigration policies? </li>
								<li> How do the Conservative Party's immigration policies compare to those of other major political parties? </li>
							</ul>
						</p>
					</section>
				</section>
				<section>
				<section>
					<h3>Contextual Chunk Headers</h3>
					<p align="left">
						Contextual chunk headers are a technique for improving the relevance and accuracy of retrieved chunks by providing additional context. By adding headers to each chunk with relevant metadata, such as the file name or section title, the similarity search can be more precise. This technique is particularly useful when dealing with large and/or complex documents where the context is essential for understanding the information.
					</p>
				</section>
				<section
					<p>
						<pre>
							<code class="language-python" data-trim data-noescape data-line-numbers="1-5|7-20|21-27|28-35|36-39">
								from llama_index.core.schema import BaseNode, TransformComponent
								from llama_index.vector_stores.faiss import FaissVectorStore
								from llama_index.core.text_splitter import SentenceSplitter
								from llama_index.core.ingestion import IngestionPipeline
								from llama_index.core import VectorStoreIndex

								class AddChunkHeader(TransformComponent):
								"""
								Transformation to be used within the ingestion pipeline.
								Cleans clutters from texts.
								"""
								def __call__(self, nodes, **kwargs) -> List[BaseNode]:
									
									for node in nodes:
										node_title = node.metadata['file_name']
										node.text = f"{node_title}\n{node.text}"
										print(node.text)
										
									return nodes

								faiss_index = faiss.IndexFlatL2(embed_dim)
								vector_store = FaissVectorStore(faiss_index=faiss_index)
								text_splitter = SentenceSplitter(
									chunk_size=chunk_size, 
									chunk_overlap=chunk_overlap
									)
								
								pipeline = IngestionPipeline(
									transformations=[
										TextCleaner(),
										text_splitter,
										AddChunkHeader(),
									],
									vector_store=vector_store,
								)
								
								nodes = pipeline.run(documents=documents)
								vector_store_index = VectorStoreIndex(nodes)
								retriever = vector_store_index.as_retriever(similarity_top_k=1)
							</code>
						</pre>
					</p>
				</section>
				<section>
					<h4>Is the retrieval actually better with the headers?</h4>
					<p>
						<pre>
							<code class="language-python" data-trim data-noescape data-line-numbers="">
								def get_similarity_score_average_from_context(context):
									similarity_scores = [c.score for c in context]
									return sum(similarity_scores) / len(similarity_scores)
								
								test_query = "Is the labour party planning to increase taxes?"
								context = retriever.retrieve(test_query)
								new_similarity_average = get_similarity_score_average_from_context(
									context
								)

								with open('../cache/faiss_index.pkl', 'rb') as f:
    								original_query_store = pickle.load(f)
								original_retriever = original_query_store.as_retriever(
									similarity_top_k=1
								)
								context = original_retriever.retrieve(test_query)
								old_similarity_average = get_similarity_score_average_from_context(
									context
								)

								print(f"New similarity average: {new_similarity_average}")
								print(f"Old similarity average: {old_similarity_average}")
							</code>
						</pre>
					</p>
					<p style="font-size: large;">
						New similarity average: 0.6926790557182427
					</p>
					<p style="font-size: large;">
						Old similarity average: 0.6819604396914913
					</p>
				</section>
				<section>
					<img src="https://media1.tenor.com/m/1ZAFl4GEOZYAAAAd/cautious-be-cautious.gif" alt="Be Cautious" style="width: 500px; height: auto;">
				</section>
				</section>
				<section>
					<section>
						<h2>Semantic Chunking</h2>
						<p align="left" style="font-size: large;">
							Traditional text splitting methods often break documents at arbitrary points, potentially disrupting the flow of information and context. Semantic chunking addresses this issue by attempting to split text at more natural breakpoints, preserving semantic coherence within each chunk.
						</p>
						<p align="left" style="font-size: large;">
							How does it work?
						</p>
						<p>
							<ul style="font-size: large;">
								<li>The text is first split into sentences using a regular expression</li>
								<li>Sentences are then grouped into overlapping sets of n sentences each</li>
								<li>Embeddings are generated for each group of n sentences using the provided embedding model</li>
								<li>The cosine dissimilarity between each group and the next is calculated</li>
								<li>If the dissimilarity exceeds a specified threshold, a new chunk is created</li>
							</ul>
					</section>
					<section>
						<h3>Base Splitter</h3>
						<p>
							<pre><code class="language-python" data-trim data-noescape data-line-numbers="12-15">
								from llama_index.vector_stores.faiss import FaissVectorStore
								from llama_index.core.text_splitter import SentenceSplitter
								from llama_index.core.ingestion import IngestionPipeline

								EMBED_DIMENSION = 512
								CHUNK_SIZE = 250
								CHUNK_OVERLAP = 25

								faiss_index = faiss.IndexFlatL2(EMBED_DIMENSION)
								vector_store = FaissVectorStore(faiss_index=faiss_index)
								
								text_splitter = SentenceSplitter(
									chunk_size=CHUNK_SIZE,
									chunk_overlap=CHUNK_OVERLAP
								)
								
								pipeline = IngestionPipeline(
									transformations=[
										TextCleaner(),
										text_splitter,
									],
									vector_store=vector_store,
								)
								
								nodes = pipeline.run(documents=documents)
								vector_store_index = VectorStoreIndex(nodes)
								query_engine = vector_store_index.as_query_engine(similarity_top_k=2)
							</code>
							</pre>
						</p>
					</section>
					<section>
						<h3>Semantic Splitter</h3>
						<p>
							<pre><code class="language-python" data-trim data-noescape data-line-numbers="1-9">
								semantic_splitter = SemanticSplitterNodeParser(
									buffer_size=1,  # number of sentences to group together 
									# when evaluating semantic similarity
									breakpoint_percentile_threshold=95,  # The percentile of 
									# cosine dissimilarity that must be exceeded between a 
									# group of sentences and the next to form a node. The 
									# smaller this number is, the more nodes will be generated
									embed_model=Settings.embed_model
								)
								pipeline = IngestionPipeline(
									transformations=[
										TextCleaner(),
										semantic_splitter,
									],
									vector_store=vector_store,
								)
							</code>
						</pre>
						</p>
				</section>
				<section>
					<h3>What is the SNP's position on the EU? (Using the base splitter)</h3>
					<p align="left" style="font-size: large;">
					DEFENDING DEMOCRACY  AND HUMAN RIGHTS
					The SNP stands on a strong record of defending Scotland’s democratic functions and institutions, and we will always stand up to promote and protect Scotland’s democracy and make sure that the people of Scotland’s voices are heard. SNP MPs will demand the UK Government:
					Give the people of Scotland a say on their future. Demand the permanent transfer of legal power to the Scottish Parliament to determine  how Scotland is governed, including the  transfer of power to enable it to legislate for  a referendum.
					End Westminster’s power grab by demanding the UK government repeal the reprehensible Internal Market Act. We are clear that UK ministers must not be able to act unilaterally across policy areas that are within devolved competencies, and will push for the Sewel Convention to be put on a proper statutory footing.
					Support abolition of the undemocratic House of Lords.
					</p>
				</section>
				<section>
					<h3>What is the SNP's position on the EU? (Using the semantic splitter)</h3>
					<p align="left" style="font-size: large;">
						DECISIONS MADE IN SCOTLAND, FOR SCOTLAND.      27SNP General Election Manifesto  2024
						SCOTLAND’S PLACE IN THE WORLD
						We want to see an independent Scotland take its place in the international community; alongside the 193 other United Nations member states, able to join the European Union, with the powers necessary to protect our citizens and prosper in the global economy. We are determined that Scotland plays a positive and progressive role in international affairs through action and leadership. SNP MPs will call on the UK Government to:
						Demand an immediate ceasefire in Gaza, release of hostages and end arms sales to Israel. We will continue to call on the UK Government to follow the lead of Ireland, Norway and Spain by immediately recognising Palestine as a state. We believe that recognising Palestine as a state in its own right is the only way to move towards a just and durable long-term peace, in the interests of both Palestinians and Israelis.
						Stand by Ukraine and continue to strongly oppose the Russian invasion. We will continue to support Ukrainians who have settled in Scotland and will press the UK Government to extend visa rights whilst the conflict continues. We will do everything in our power  to see the restoration of peace and ensure Ukraine’s sovereignty, democracy, independence and territorial integrity is maintained. We will continue to back military support from the United Kingdom to Ukraine and will press the UK Government to ensure the sanctions regime against Russia is effective. Scrap Trident and invest the billions spent funding these immoral weapons in public services, like our NHS and schools and adequately funding conventional defence. The SNP has never and will never support the retention or renewal of Trident, and will press the UK government to meet their international obligations on nuclear disarmament.
						Connect Scotland’s diaspora across  the globe by working with national and international partners to implement the  Scottish Connections Framework. Agree an EU-wide youth mobility scheme.  Despite the EU proposing a post-Brexit mobility scheme for young people, both the Tories and Labour rejected it within hours. The UK Government must stop failing young people and agree a scheme so that young people can benefit from the opportunities that living, working and studying in the EU can bring.
						Immediately restore and maintain the UK international aid budget to 0.7%.
					</p>
				</section>
				</section>
				<!-- <section>
					<h3>Other Methods for improving RAG</h3>
					<ul>
						<li>
							Proposition Chunking
						</li>
						<li>
							Semantic Chunking
						</li>
						<li>
							Fusion Retrieval
						</li>
						<li>
							Intelligent Reranking
						</li>
						<li>
							Hirearchical Indexing
						</li>
						<li>
							Ensemble Retrieval
						</li>
						<li>
							Knowledge Graph Integration
						</li>
						<li>
							Agent Assisted Retrieval
						</li>
					</ul>
				</section> -->
				<!-- <section>
					<section>
						<section>
							<img src="https://substackcdn.com/image/fetch/w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F0cc9ca6d-a73c-47ff-a1a6-b88b7a0b651b_387x585.jpeg" alt="Evaluation" style="width: auto; height: 500px;">
						</section>
						<h2>Evaluation</h2>
						<p align="left">
							Evaluating the performance of a RAG system is essential to ensure that it meets the desired objectives and provides accurate and relevant information. A few guidelines for evaluating a RAG system include:
						</p>
						</section>
						<section>
							<h3>Benchmark Creation</h3>
								<ul>
									<li>Collecting questions that accurately represent real-world user queries</li>
									<li>Involving domain experts familiar with the knowledge base</li>
									<li>Ensuring the benchmark covers the full spectrum of expected query types</li>
								</ul>
						</section>
						<section>
							<h3>Evaluation Metrics</h3>
								<ul style="font-size: large;">
									<li>Different metrics for three levels: retrieval, generation, and indexing</li>
									<li>Sub-optimal performance in an earlier stage can affect the overall performance</li>
									<li>Retrieval evaluation - Context Precision, Context Recall, Normalized Discounted Cumulative Gain (NDCG)</li>
									<li>Generation evaluation - Faithfulness, Answer Relevancy</li>
									<li>Indexing evaluation - Recall, Approximate Nearest Neighbors (ANN)</li>
								</ul>
						</section>
						<section>
							<h3>LLM-Based Evaluation</h3>
								<ul>
									<li>Zero-shot LLM evaluation: Using prompt templates to rate relevance and quality</li>
									<li>Few-shot LLM evaluation: Providing examples to guide the LLM's assessment</li>
								</ul>
						</section>
						<section>
							<h3>Parameter Space Optimization</h3>
								<ul>
									<li>Indexing parameters: chunk size, embedding model</li>
									<li>Retrieval parameters: top-k results, preprocessing model</li>
									<li>Generation parameters: LLM selection, prompt engineering</li>
								</ul>
					</section>
					<section>
						<h3>Using Deepeval for evaluation</h3>
						<p align="left" style="font-size: large;">
							Deepeval is a tool that provides a comprehensive evaluation framework for RAG systems. It allows users to define custom evaluation metrics, compare different models, and generate detailed reports on system performance.
						</p>
						<p align="left" style="font-size: large;">
							First we need to set a list of questions and answers to evaluate the system, This is our ground truth. We will add it as a JSON file which looks like this:
						</p>
						<p>
							<pre>
								<code class="language-json" data-trim data-noescape>
									[
										{
											"question": "What is the SNP's policy on climate change?",
											"answer": "The SNP's policy on climate change includes banning 
											new coal licenses, ensuring fair funding for climate 
											initiatives, establishing a Four Nations Climate Response 
											Group to meet net-zero targets, devolving powers for a bespoke 
											migration system, mitigating the harm of Brexit on 
											productivity, providing sustainable funding for farming, and 
											giving Scotland its rightful share of marine funding."
										},
										{
											"question": "What is the Conservative Party......
								</code>
							</pre>
						</p>
					</section>
					<section>
						<h4>Load the questions and answers as test cases</h4>
						<p>
							<pre>
								<code class="language-python" data-trim data-noescape data-line-numbers="">
									from deepeval.test_case import LLMTestCaseParams, LLMTestCase

									with open('test_cases.json', 'r') as f:
										qsandas = json.load(f)

									test_cases = []
									for qna in qsandas:
										question = qna["question"]
										gt_answer = qna["answer"]
										generated_answer = retriever.retrieve(question)
										retrieved_document = retriever.retrieve(question)
										test_cases.append(
											LLMTestCase(
												input=question,
												expected_output=gt_answer,
												actual_output=generated_answer,
												retrieval_context=retrieved_document
											)
										)
								</code>
							</pre>
					</section>
					<section>
						<h3>Define Evaluation Metrics</h3>
						<p>
							<pre>
								<code class="language-python" data-trim data-noescape data-line-numbers="1-4|6-20|21-30|31-45">
									from deepeval.metrics import (
										GEval, FaithfulnessMetric, ContextualRelevancyMetric
									)
									LLM_MODEL = "gpt-4o"

									# G-Eval is a framework that uses LLMs with chain-of-thoughts 
									# (CoT) to evaluate LLM outputs based on ANY custom criteria
									correctness_metric = GEval(
										name="Correctness",
										model=LLM_MODEL,
										evaluation_params=[
											LLMTestCaseParams.EXPECTED_OUTPUT,
											LLMTestCaseParams.ACTUAL_OUTPUT
										],
										evaluation_steps=[
											"Determine whether the actual output is factually 
											correct based on the expected output."
										],
									)

									# The faithfulness metric measures the quality of your RAG 
									# pipeline's generator by evaluating whether the 
									# actual_output factually aligns with the contents of your 
									# retrieval_context
									faithfulness_metric = FaithfulnessMetric(
										threshold=0.7,
										model=LLM_MODEL,
										include_reason=False  # Set to True for detailed explanations
									)

									# The contextual relevancy metric measures the quality of your 
									# RAG pipeline's retriever by evaluating the overall relevance 
									# of the information presented in your retrieval_context for a 
									# given input
									relevance_metric = ContextualRelevancyMetric(
										threshold=1,
										model=LLM_MODEL,
										include_reason=True
									)
								</code>
							</pre>
					</section>
					<section>
						<h3>Evaluation Example Report No. 1</h3>
						<p>
							<pre>
								<code class="markdown" data-trim data-noescape>
									Metrics Summary

									- ✅ Correctness (GEval) (score: 1.0, threshold: 0.5, strict: False, evaluation model: gpt-4o, reason: The actual output is factually correct as it matches the expected output verbatim regarding the Green party's promise., error: None)
									- ✅ Faithfulness (score: 1.0, threshold: 0.7, strict: False, evaluation model: gpt-4o, reason: None, error: None)
									- ✅ Contextual Relevancy (score: 1.0, threshold: 1.0, strict: False, evaluation model: gpt-4o, reason: The score is 1.00 because the context perfectly aligns with the input. Great job!, error: None)

									For test case:

									- input: By how much does the Green party promise to increase the disability benefits?
									- actual output: The Green party promises to increase disability benefits by 5%.
									- expected output: The Green party promises to increase disability benefits by 5%.
									- context: None
									- retrieval context: ['• Increase all disability benefits by 5%.\n• Ensure that pensions...']
								</code>
							</pre>
					</section>
					<section>
						<h3>Evaluation Example Report No. 2</h3>
						<p>
							<pre>
								<code class="markdown" data-trim data-noescape>
									Metrics Summary

									- ❌ Correctness (GEval) (score: 0.11393602840389586, threshold: 0.5, strict: False, evaluation model: gpt-4o, reason: The actual output does not mention ending tax breaks for private schools, which is the method of financing in the expected output. It instead discusses reviewing bursaries and other unrelated strategies., error: None)
									- ✅ Faithfulness (score: 1.0, threshold: 0.7, strict: False, evaluation model: gpt-4o, reason: None, error: None)
									- ❌ Contextual Relevancy (score: 0.0, threshold: 1.0, strict: False, evaluation model: gpt-4o, reason: The score is 0.00 because the context discusses the current government's achievements and plans in education, but it does not address Labour's specific financing plans for recruiting 6500 new teachers., error: None)
								  
								  	For test case:
								  
									- input: How is the Labour party planning to finance recruiting 6500 new teachers?
									- actual output: The Labour party is planning to finance recruiting 6500 new teachers by reviewing the way bursaries are allocated, addressing retention issues, and updating the Early Career Framework to ensure new teachers entering the classroom have or are working towards Qualified Teacher Status.
									- expected output: The Labour party is planning to finance recruiting 6500 new teachers by ending tax breaks for private schools.
									- context: None
									- retrieval context: ['26  Education is the closest thing we have to a silver bullet, which is why since 2010 we have focused on driving up standards in education. English children are now the best readers in the Western world and are 11th in the world for maths, up from 27th when Labo
								</code>
							</pre>
					</section>
				</section> -->
				<!-- <section>
					<h4>To fine-tune or not to fine-tune?</h4>
					<p align="left" style="font-size: large;">
						Fine-tuning a language model is a common practice in natural language processing to adapt a pre-trained model to a specific task or domain. While fine-tuning can improve the performance of a model on a specific task, it also comes with its own set of benefits and challenges.
					<p>
						Benefits
					</p>
					<ul style="font-size: large;">
						<li>Generally outperforms non-fine-tuned models, especially for specialized tasks</li>
						<li>More consistent responses within the fine-tuned domain</li>
						<li>Potentially lower risk within the fine-tuned domain</li>
					</ul>
					<p>
						Challenges
					</p>
					<ul style="font-size: large;">
						<li>Requires a large amount of high-quality training data</li>
						<li>May overfit to the training data, leading to poor generalization</li>
						<li>Time-consuming and computationally expensive</li>
						<li>Might require periodic retraining to stay current</li>
					</ul>
				</p>
				</section>
				<section>
					<h3>
						What's next for RAG systems?
					</h3>
					<p>
						<ul>
							<li>Improving the scalability and efficiency of RAG systems</li>
							<li>Integrating multi-modal information retrieval</li>
							<li>Longer context windows will make the answers more accurate</li>
							<li>Addressing privacy and security concerns</li>
						</ul>
					</p>
				</section> -->
				<section>
					<h3>What have we covered?</h3>
					<ul>
						<li>What is RAG good for?</li>
						<li>Limitations of RAG systems</li>
						<li>How to improve RAG systems</li>
						<!-- <li>Evaluation is crucial for assessing the performance of a RAG system, and tools like Deepeval can help with this process</li>
						<li>Fine-tuning and future directions for RAG systems</li> -->
					</ul>
				</section>
				<section>
					<img src="https://media3.giphy.com/media/v1.Y2lkPTc5MGI3NjExYWQ2czlsNmxveW84cjQ5MzAyajJjajJ6YXh1cGxucXV4bnA4ZXh2OSZlcD12MV9pbnRlcm5hbF9naWZfYnlfaWQmY3Q9Zw/XHVmD4RyXgSjd8aUMb/giphy.webp" alt="The End" style="width: 400px; height: auto;">
					<p align="center" style="font-size: large;">
						For more RAG techniques, check out this GitHub repository: <a href="https://github.com/NirDiamant/RAG_Techniques/">github.com/NirDiamant/RAG_Techniques/</a>
					</p>
					<p align="center" style="font-size: large;">
						Thank you for your attention! Connect with me on LinkedIn by scanning the QR code below or visiting my profile at <a href="https://www.linkedin.com/in/natan-mish/">linkedin.com/in/natan-mish/</a>
					</p>
					<p>
						<img src="images/QR_code.JPG" alt="QR Code" style="width: 150px; height: auto;">
					</p>
				</section>
			</div>
		</div>

		<script src="dist/reveal.js"></script>
		<script src="plugin/notes/notes.js"></script>
		<script src="plugin/markdown/markdown.js"></script>
		<script src="plugin/highlight/highlight.js"></script>
		<script>
			// More info about initialization & config:
			// - https://revealjs.com/initialization/
			// - https://revealjs.com/config/
			Reveal.initialize({
				hash: true,

				// Learn about plugins: https://revealjs.com/plugins/
				plugins: [ RevealMarkdown, RevealHighlight, RevealNotes ]
			});
		</script>
	</body>
</html>
